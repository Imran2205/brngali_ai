{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bangla-ai-64pix/x_train_banglaHW_64pix.pickle\n",
      "/kaggle/input/bangla-ai-64pix/y_train_consonant_banglaHW_64pix.pickle\n",
      "/kaggle/input/bangla-ai-64pix/y_train_vowel_banglaHW_64pix.pickle\n",
      "/kaggle/input/bangla-ai-64pix/y_test_consonant_banglaHW_64pix.pickle\n",
      "/kaggle/input/bangla-ai-64pix/y_test_vowel_banglaHW_64pix.pickle\n",
      "/kaggle/input/bangla-ai-64pix/x_test_banglaHW_64pix.pickle\n",
      "/kaggle/input/bangla-ai-64pix/y_train_root_banglaHW_64pix.pickle\n",
      "/kaggle/input/bangla-ai-64pix/y_test_root_banglaHW_64ix.pickle\n",
      "/kaggle/input/balanced-32-bengali-dataset/x_test_32_vowel_banglaHW_list_oversampled.pickle\n",
      "/kaggle/input/balanced-32-bengali-dataset/x_train_32_vowel_banglaHW_list_oversampled.pickle\n",
      "/kaggle/input/balanced-32-bengali-dataset/y_test_32_vowel_banglaHW_list_oversampled.pickle\n",
      "/kaggle/input/balanced-32-bengali-dataset/x_train_32_root_banglaHW_list_oversampled.pickle\n",
      "/kaggle/input/balanced-32-bengali-dataset/x_test_32_root_banglaHW_list_oversampled.pickle\n",
      "/kaggle/input/balanced-32-bengali-dataset/y_train_32_consonant_banglaHW_list_oversampled.pickle\n",
      "/kaggle/input/balanced-32-bengali-dataset/y_test_32_root_banglaHW_list_oversampled.pickle\n",
      "/kaggle/input/balanced-32-bengali-dataset/y_test_32_consonant_banglaHW_list_oversampled.pickle\n",
      "/kaggle/input/balanced-32-bengali-dataset/x_test_32_consonant_banglaHW_list_oversampled.pickle\n",
      "/kaggle/input/balanced-32-bengali-dataset/x_train_32_consonant_banglaHW_list_oversampled.pickle\n",
      "/kaggle/input/balanced-32-bengali-dataset/y_train_32_vowel_banglaHW_list_oversampled.pickle\n",
      "/kaggle/input/balanced-32-bengali-dataset/y_train_32_root_banglaHW_list_oversampled.pickle\n",
      "/kaggle/input/bengaliai-cv19/train.csv\n",
      "/kaggle/input/bengaliai-cv19/train_image_data_2.parquet\n",
      "/kaggle/input/bengaliai-cv19/test_image_data_1.parquet\n",
      "/kaggle/input/bengaliai-cv19/class_map.csv\n",
      "/kaggle/input/bengaliai-cv19/train_image_data_3.parquet\n",
      "/kaggle/input/bengaliai-cv19/test_image_data_2.parquet\n",
      "/kaggle/input/bengaliai-cv19/train_image_data_1.parquet\n",
      "/kaggle/input/bengaliai-cv19/test_image_data_0.parquet\n",
      "/kaggle/input/bengaliai-cv19/test.csv\n",
      "/kaggle/input/bengaliai-cv19/train_image_data_0.parquet\n",
      "/kaggle/input/bengaliai-cv19/sample_submission.csv\n",
      "/kaggle/input/bengaliai-cv19/test_image_data_3.parquet\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "import pyarrow as pa\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import clone_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.core import Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import gc\n",
    "import keras\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pickle_in = open(\"/kaggle/input/bengali-ai/x_banglaHW_list.pickle\", \"rb\")\\ntrainX = pickle.load(pickle_in)\\npickle_in.close()\\n\\npickle_in = open(\"/kaggle/input/bengali-ai/y_banglaHW_list_root.pickle\", \"rb\")\\ntrainYRoot = pickle.load(pickle_in)\\npickle_in.close()\\n\\npickle_in = open(\"/kaggle/input/bengali-ai/y_banglaHW_list_vowel.pickle\", \"rb\")\\ntrainYVowel = pickle.load(pickle_in)\\npickle_in.close()\\n\\npickle_in = open(\"/kaggle/input/bengali-ai/y_banglaHW_list_consonant.pickle\", \"rb\")\\ntrainYConsonant = pickle.load(pickle_in)\\npickle_in.close()'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_in = open(\"/kaggle/input/bangla-ai-64pix/x_train_banglaHW_64pix.pickle\", \"rb\")\n",
    "xTrainData = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open(\"/kaggle/input/bangla-ai-64pix/x_test_banglaHW_64pix.pickle\", \"rb\")\n",
    "xTestData = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open(\"/kaggle/input/bangla-ai-64pix/y_train_root_banglaHW_64pix.pickle\", \"rb\")\n",
    "yTrainData = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open(\"/kaggle/input/bangla-ai-64pix/y_test_root_banglaHW_64ix.pickle\", \"rb\")\n",
    "yTestData = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "'''pickle_in = open(\"/kaggle/input/bangla-ai-dataset-new/x_validation_banglaHW_list.pickle\", \"rb\")\n",
    "xTestData = pickle.load(pickle_in) \n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open(\"/kaggle/input/bangla-ai-dataset-new/y_root_train_banglaHW_list.pickle\", \"rb\")\n",
    "yTrainRootData = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open(\"/kaggle/input/bangla-ai-dataset-new/y_root_validation_banglaHW_list.pickle\", \"rb\")\n",
    "yTestRootData = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open(\"/kaggle/input/bangla-ai-dataset-new/y_vowel_train_banglaHW_list.pickle\", \"rb\")\n",
    "yTrainVowelData = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open(\"/kaggle/input/bangla-ai-dataset-new/y_vowel_validation_banglaHW_list.pickle\", \"rb\")\n",
    "yTestVowelData = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open(\"/kaggle/input/bangla-ai-dataset-new/y_consonant_train_banglaHW_list.pickle\", \"rb\")\n",
    "yTrainConsonantData = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open(\"/kaggle/input/bangla-ai-dataset-new/y_consonant_validation_banglaHW_list.pickle\", \"rb\")\n",
    "yTestConsonantData = pickle.load(pickle_in)\n",
    "pickle_in.close()'''\n",
    "\n",
    "'''pickle_in = open(\"/kaggle/input/bengali-ai/x_banglaHW_list.pickle\", \"rb\")\n",
    "trainX = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open(\"/kaggle/input/bengali-ai/y_banglaHW_list_root.pickle\", \"rb\")\n",
    "trainYRoot = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open(\"/kaggle/input/bengali-ai/y_banglaHW_list_vowel.pickle\", \"rb\")\n",
    "trainYVowel = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open(\"/kaggle/input/bengali-ai/y_banglaHW_list_consonant.pickle\", \"rb\")\n",
    "trainYConsonant = pickle.load(pickle_in)\n",
    "pickle_in.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xTrainData[1000])\n",
    "plt.show\n",
    "CLS = (yTrainData.shape[1])\n",
    "print(CLS)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190798, 64, 64, 1)\n",
      "(10042, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = xTrainData.shape[1]\n",
    "xTrainData = np.array(xTrainData).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "xTestData = np.array(xTestData).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "print(xTrainData.shape)\n",
    "print(xTestData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190798, 168)\n",
      "(10042, 168)\n"
     ]
    }
   ],
   "source": [
    "print(yTrainData.shape)\n",
    "print(yTestData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xTrainData = xTrainData/np.float32(255.0)\n",
    "xTestData = xTestData/np.float32(255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190798, 64, 64, 1)\n",
      "(10042, 64, 64, 1)\n",
      "(190798, 168)\n",
      "(10042, 168)\n"
     ]
    }
   ],
   "source": [
    "yTrainData = np.array(yTrainData)\n",
    "yTestData = np.array(yTestData)\n",
    "print((xTrainData.shape))\n",
    "print((xTestData.shape))\n",
    "print((yTrainData.shape))\n",
    "print((yTestData.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallerVGGNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes, finalAct=\"softmax\"):\n",
    "        # initialize the model along with the input shape to be\n",
    "        # \"channels last\" and the channels dimension itself\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "\n",
    "        # if we are using \"channels first\", update the input shape\n",
    "        # and channels dimension\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\",\n",
    "            input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "        model.add(Dropout(0.25))\n",
    "        # (CONV => RELU) * 2 => POOL\n",
    "        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        # (CONV => RELU) * 2 => POOL\n",
    "        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1024))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # use a *softmax* activation for single-label classification\n",
    "        # and *sigmoid* activation for multi-label classification\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(finalAct))\n",
    "\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"learning_rate_reduction_vowel = ReduceLROnPlateau(monitor='dense_2_accuracy', \\n                                            patience=2, \\n                                            verbose=1,\\n                                            factor=0.5, \\n                                            min_lr=0.000001)\\nlearning_rate_reduction_consonant = ReduceLROnPlateau(monitor='dense_3_accuracy', \\n                                            patience=2, \\n                                            verbose=1,\\n                                            factor=0.5, \\n                                            min_lr=0.000001)\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SmallerVGGNet.build(\n",
    "    width=xTrainData.shape[2], height=xTrainData.shape[1],\n",
    "    depth=xTrainData.shape[3], classes=CLS,\n",
    "    finalAct=\"softmax\")\n",
    "\n",
    "INIT_LR = 1e-3\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / epochs)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased\n",
    "'''learning_rate_reduction = ReduceLROnPlateau(monitor='dense_1_accuracy', \n",
    "                                            patience=2, \n",
    "                                            verbose=1,\n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.000001)'''\n",
    "'''learning_rate_reduction_vowel = ReduceLROnPlateau(monitor='dense_2_accuracy', \n",
    "                                            patience=2, \n",
    "                                            verbose=1,\n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.000001)\n",
    "learning_rate_reduction_consonant = ReduceLROnPlateau(monitor='dense_3_accuracy', \n",
    "                                            patience=2, \n",
    "                                            verbose=1,\n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.000001)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=8,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.2, # Randomly zoom image \n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "# This will just calculate parameters required to augment the given data. This won't perform any augmentations\n",
    "#datagen.fit(xTrainData)\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "#gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction = 0.25)\n",
    "\n",
    "#sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1490/1490 [==============================] - 124s 83ms/step - loss: 4.1765 - accuracy: 0.1179 - val_loss: 15.3988 - val_accuracy: 0.0049\n",
      "Epoch 2/50\n",
      "1490/1490 [==============================] - 118s 79ms/step - loss: 1.8133 - accuracy: 0.5105 - val_loss: 0.8921 - val_accuracy: 0.7465\n",
      "Epoch 3/50\n",
      "1490/1490 [==============================] - 119s 80ms/step - loss: 1.2211 - accuracy: 0.6643 - val_loss: 0.6794 - val_accuracy: 0.8101\n",
      "Epoch 4/50\n",
      "1490/1490 [==============================] - 120s 81ms/step - loss: 1.0071 - accuracy: 0.7223 - val_loss: 0.7012 - val_accuracy: 0.8027\n",
      "Epoch 5/50\n",
      "1490/1490 [==============================] - 118s 79ms/step - loss: 0.8923 - accuracy: 0.7523 - val_loss: 0.4945 - val_accuracy: 0.8605\n",
      "Epoch 6/50\n",
      "1490/1490 [==============================] - 117s 79ms/step - loss: 0.8112 - accuracy: 0.7741 - val_loss: 0.4774 - val_accuracy: 0.8647\n",
      "Epoch 7/50\n",
      "1490/1490 [==============================] - 118s 79ms/step - loss: 0.7596 - accuracy: 0.7889 - val_loss: 0.3835 - val_accuracy: 0.8912\n",
      "Epoch 8/50\n",
      "1490/1490 [==============================] - 118s 79ms/step - loss: 0.7121 - accuracy: 0.8015 - val_loss: 0.8758 - val_accuracy: 0.7582\n",
      "Epoch 9/50\n",
      "1490/1490 [==============================] - 117s 78ms/step - loss: 0.6800 - accuracy: 0.8098 - val_loss: 0.4063 - val_accuracy: 0.8842\n",
      "Epoch 10/50\n",
      "1490/1490 [==============================] - 119s 80ms/step - loss: 0.4935 - accuracy: 0.8591 - val_loss: 0.2710 - val_accuracy: 0.9209\n",
      "Epoch 22/50\n",
      "1490/1490 [==============================] - 117s 79ms/step - loss: 0.4182 - accuracy: 0.8790 - val_loss: 0.3461 - val_accuracy: 0.8966\n",
      "Epoch 35/50\n",
      "1490/1490 [==============================] - 116s 78ms/step - loss: 0.3763 - accuracy: 0.8903 - val_loss: 0.2223 - val_accuracy: 0.9364\n",
      "Epoch 48/50\n",
      " 170/1490 [==>...........................] - ETA: 1:36 - loss: 0.3648 - accuracy: 0.8941"
     ]
    }
   ],
   "source": [
    "history =  model.fit_generator(\n",
    "    datagen.flow(xTrainData, yTrainData, batch_size=batch_size),\n",
    "    validation_data = (xTestData, yTestData),\n",
    "    steps_per_epoch=len(xTrainData) // batch_size,\n",
    "    epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9314\n",
      "10042\n",
      "0.927504481179048\n"
     ]
    }
   ],
   "source": [
    "prediction=model.predict(xTestData)\n",
    "correct = 0\n",
    "total = 0\n",
    "for x in range(xTestData.shape[0]):\n",
    "    \n",
    "    guessed = np.argmax(prediction[x])\n",
    "\n",
    "    actual = np.argmax(yTestData[x])\n",
    "    if(guessed == actual):\n",
    "        correct += 1\n",
    "    total+=1\n",
    "print(correct)\n",
    "print(total)\n",
    "print(correct/total)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_b_hw_root.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "# plot accuracy\n",
    "plt.subplot(212)\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "# save plot to file\n",
    "\n",
    "plt.savefig('_plot.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_root.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
